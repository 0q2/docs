# Recommend using Gorse

Recommendation consists of two phases: matching and ranking. The number of items in a recommender system is usually very large, and it is not practical to ranking all items. Therefore, the matching phase is needed to filter out candidate items from all items, and then the ranking model utilizes the item and user labels for more accurate ranking.

<center><img src="img/dataflow.png" height="180"></center>

## Matching Strategies

There are currently three matching strategies in the system: latest items, recently popular items and collaborative filtering. In fact, recall strategies are not limited to these three, but can also be based on the user's interested tags, items similar to the user's favorite items, etc. Feel free to discuss in [issues](https://github.com/zhenghaoz/gorse/issues).

- **Latest Items:** Add the latest items directly to the ranking phase so that new items are given the opportunity to be exposed.

- **Recent Popular Items:** Users are more likely to like popular items, but we need to set a time limit to avoid recommending "outdated" popular items.

- **Collaborative Filtering:** Use collaborative filtering to filter candidate items from the entire item pool. Since collaborative filtering does not use item labels, it is less computationally intensive and suitable for matching scenarios. Three collaborative filtering models, BPR, ALS and CCD, are implemented in the system.

| Model | Paper |
| ---- | ------------------------------------------------------------ |
| ALS  |  |
| BPR  | Rendle, Steffen, et al. "BPR: Bayesian personalized ranking from implicit feedback." arXiv preprint arXiv:1205.2618 (2012). |
| CCD |  |

## Ranking Mechanism

The ranking model takes into account labels of the items, especially for new items, where the label is the basis for deciding whether to push the new item to the user or not. The ranking model for this system is factorization machines.

| Model | Paper |
| - | - |
| FM | |

## Recommendation

Recommended items come from multiple sources through multiple stages. Non-personalized recommendations (popular/latest/similar) are generated by the master node. Offline personalized recommendations are generated by worker nodes while online personalized recommendations are generated by server nodes.

### Popular Items

Items with maximum number of users will are collected. To avoid popular items resist on the top list, `popular_window` restricts that timestamps of collected items must be after `popular_window` days ago. There will be no timestamp restriction if `popular_window` is `0`.

### Latest Items

Items with latest timestamps are collected. Items won't be added to latest items collection if their timestamp is empty.

### Similar Items

For each item, top n (n equals `cache_size`) similar items are collected. In current implementation, the similarity between items are the number of common users of two items[^6].

### Offline Recommendation

### Online Recommendation

The online recommendation in the server node consists of three stages:

1. Load offline recommendation from cache, remove read items.
2. If the number of offline recommendations is less than required, collect items similar to these items in the user's histrical feedbacks. Read items are removed as well.
3. If the number of recommendations is still less than required, collect items from `fallback_recommend` (latest items or popular items). Read items are removed.

## Model Update

There are two kinds of models in Gorse, but the training and hyperparameters optimization procedures are quite the same. 

### Model Training

Model training are done by the master node, as well as model search. The master node pull data from database and fit ranking model and CTR model periodically.

> - For every `fit_jobs` minutes:
>   - Pull data from database.
>       - Train model with hyperparameters found by model search using `fit_jobs` jobs.

### Model Search

There are many hyperparameters for each recommendation model in Gorse. However, it is hard to configuare these hyperparameters manually even for machine learning experts. To help users get rid of hyperparameters tuning, Gorse integrates random search[^1] for hyperparameters optimization. The procedure of model search is as following:

> - For every `search_period` minutes:
>   - Pull data from database.
>   - For every recommender models:
>       - For `search_trials` trials:
>           - Sample a hyperparameter combination.
>           - Train model with sampled hyperparameters by `search_epoch` epoches and `search_jobs` jobs.
>           - Update best model.


[^6]: Zhang, Zhenghao, et al. "SANS: Setwise Attentional Neural Similarity Method for Few-Shot Recommendation." DASFAA (3). 2021.

[^1]: Bergstra, James, and Yoshua Bengio. "Random search for hyper-parameter optimization." Journal of machine learning research 13.2 (2012).

[^2]: Rendle, Steffen. "Factorization machines." *2010 IEEE International Conference on Data Mining*. IEEE, 2010. 

[^3]: Hu, Yifan, Yehuda Koren, and Chris Volinsky. "Collaborative filtering for implicit feedback datasets." *2008 Eighth IEEE International Conference on Data Mining*. Ieee, 2008.

[^4]: He, Xiangnan, et al. "Fast matrix factorization for online recommendation with implicit feedback." Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. 2016.

[^5]: Rendle, Steffen, et al. "BPR: Bayesian personalized ranking from implicit feedback." Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. 2009.
